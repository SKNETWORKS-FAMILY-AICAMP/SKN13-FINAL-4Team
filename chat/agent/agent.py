# backend/chat/agent/agent.py
import asyncio
import uuid
from datetime import datetime
from typing import Callable
import time
from .state import AgentState
from .classifiers import LiteClassifier, EmotionClassifier
from .topic import TopicThreading
from .queue_manager import QueueManager
from .responder import Responder
from .pipeline import GraphPipeline
from .idle import IdleManager
from ..response_manager import ResponseManager
from ..activity_manager import ActivityManager
from .story import StoryRepository
from .db import UserDB, Utils
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import HumanMessage
from ..services.persona_loader import load_persona_profile

class LoveStreamerAgent:
    """í†µí•© ì—ì´ì „íŠ¸"""
    def __init__(self, api_key: str, story_repo: StoryRepository, streamer_id: str = None):
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0.2, api_key=api_key)
        self.fast_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=api_key)
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=api_key)
        self.lite = LiteClassifier(self.fast_llm)
        self.topic = TopicThreading(self.fast_llm, self.embeddings)
        self.queue = QueueManager(self.topic, trigger_graph_cb=self.trigger_graph_async, broadcast_cb=self.broadcast_queue_state)
        self.emotion_cls = EmotionClassifier(self.fast_llm)
        self.responder = Responder(self, self.llm, self.emotion_cls, streamer_id=streamer_id) # self(agent) ì „ë‹¬
        self.idle = IdleManager(self, self.llm, self.queue, story_repo, self.responder, streamer_id=streamer_id) # self(agent) ì „ë‹¬
        self.graph = GraphPipeline(self.responder, self.queue, UserDB()).build()
        self.superchat_q = asyncio.Queue()
        self.streamer_id = streamer_id
        self.persona_profile = {} # í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ì„ ì €ì¥í•  ë³€ìˆ˜
        # Managers (ì™¸ë¶€ì—ì„œ ì£¼ì…)
        self.response_manager: ResponseManager | None = None
        self.activity_manager: ActivityManager | None = None
        self.idle_task = None
        self.superchat_task = None

        self.idle.set_graph_trigger(self.trigger_graph_async)
        self.idle.set_bootstrap_helpers(
            topic_label_fn=lambda: self.topic.topic_ctx.get("active_label") or "",
            bootstrap_fn=self._bootstrap_topic_from_tail
        )
        self.graph.idle_mgr = self.idle

    @classmethod
    async def create(cls, api_key: str, story_repo: StoryRepository, streamer_id: str = None):
        """ë¹„ë™ê¸° ì´ˆê¸°í™”ë¥¼ í¬í•¨í•œ ì—ì´ì „íŠ¸ ìƒì„± íŒ©í† ë¦¬ ë©”ì„œë“œ"""
        agent = cls(api_key, story_repo, streamer_id)
        await agent._load_persona()
        return agent

    def run(self):
        """ì—ì´ì „íŠ¸ì˜ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
        if self.idle_task is None:
            self.idle_task = asyncio.create_task(self.idle.idle_loop())
            print(f"âœ… [{self.streamer_id}] IdleManager task started.")
        if self.superchat_task is None:
            self.superchat_task = asyncio.create_task(self.superchat_worker_coro())
            print(f"âœ… [{self.streamer_id}] Superchat worker task started.")

    def shutdown(self):
        """ì—ì´ì „íŠ¸ì˜ ëª¨ë“  ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."""
        print(f"ğŸ—‘ï¸ [{self.streamer_id}] LoveStreamerAgent shutting down...")
        if self.idle_task:
            self.idle_task.cancel()
            self.idle_task = None
            print(f"ğŸ›‘ [{self.streamer_id}] IdleManager task cancelled.")
        if self.superchat_task:
            self.superchat_task.cancel()
            self.superchat_task = None
            print(f"ğŸ›‘ [{self.streamer_id}] Superchat worker task cancelled.")

    async def _load_persona(self):
        """DBì—ì„œ í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ì„ ë¡œë“œí•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ì— ì €ì¥"""
        if self.streamer_id:
            self.persona_profile = await load_persona_profile(self.streamer_id)
            if self.persona_profile:
                print(f"âœ… [{self.streamer_id}] í˜ë¥´ì†Œë‚˜ ë¡œë“œ ì™„ë£Œ.")
            else:
                print(f"âš ï¸ [{self.streamer_id}] í˜ë¥´ì†Œë‚˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")

    def _compact_result_from_state(self, state: dict) -> dict:
        return {
            "assistant_text": Utils.text_of(state["messages"][-1]) if state.get("messages") else None,
            "emotion": state.get("assistant_emotion"),
            "categories": state.get("categories", []),
        }

    async def on_new_input_async(self, input_msg: dict):
        # í™œë™ ë§ˆí‚¹ (ê°€ëŠ¥í•œ ì¦‰ì‹œ)
        try:
            if self.activity_manager:
                content = input_msg.get("content", "") or ""
                self.activity_manager.mark_activity(
                    source="agent_input",
                    details=f"len={len(content)} type={input_msg.get('type','')}",
                    user_info={"user_id": input_msg.get("user_id")}
                )
        except Exception:
            pass
        msg_type = input_msg.get("type", "normal")
        content = input_msg.get("content", "")
        msg_id = input_msg.get("msg_id") or str(uuid.uuid4())
        if "msg_id" not in input_msg:
            input_msg = {**input_msg, "msg_id": msg_id}

        if msg_type == "superchat" and content:
            self.queue.mark_event()
            await self.superchat_q.put(dict(input_msg))
            return {"routed": "superchat_async_worker", "state": None, "result": None}

        if msg_type == "normal" and content:
            self.queue.mark_event()
            await self.queue.enqueue_general_chat(input_msg, self.lite)
            await asyncio.sleep(0.1)
            await self.queue.wait_graph_idle(1.0)

        state = self.graph.agent_state
        return {"state": state, "result": self._compact_result_from_state(state)}

    def trigger_graph_async(self, reason: str = ""):
        print(f"[trigger_graph_async] Triggered (reason={reason})")
        # ì´ë¯¸ ê·¸ë˜í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ë¬´ì‹œí•˜ì—¬ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
        if not self.queue.is_busy():
            asyncio.create_task(self.graph.run_one_turn())
        else:
            print(f"[trigger_graph_async] Graph is busy, ignoring trigger: {reason}")

    def _bootstrap_topic_from_tail(self):
        with self.queue._q_lock:
            gq = self.queue.general_queue
            if not self.topic.topic_ctx.get("active_tid") and len(gq) > 0:
                last = gq[-1]
                tid, label = last.get("thread_id"), last.get("topic", "ì¼ë°˜")
                self.topic.topic_ctx.update({"active_tid": tid, "active_label": label, "score": self.topic.TOPIC_SCORE_TRIGGER, "started_at": time.monotonic()})

    async def superchat_worker_coro(self):
        """Superchat queue consumer coroutine"""
        while True:
            try:
                msg = await self.superchat_q.get()
                print(f"ğŸ’° Handling superchat: {msg}")
                
                # í›„ì› ë©”ì‹œì§€ë¥¼ AI ì‘ë‹µ ìƒì„± íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì „ë‹¬
                superchat_state = {
                    "messages": [],
                    "type": "superchat", 
                    "categories": ["í›„ì›", "ê°ì‚¬"], 
                    "best_chat": msg.get("content", ""),
                    "user_id": msg.get("user_id", "anonymous"),
                    "chat_date": msg.get("chat_date", datetime.now().strftime("%Y-%m-%d %H:%M:%S")),
                    "db_greeting_info": {"exists": False},
                    "__no_selection": False,
                    "assistant_emotion": "grateful",
                    "msg_id": None,
                    "metadata": msg.get("metadata", {})
                }
                
                # í›„ì› ì „ìš© ì‘ë‹µ ìƒì„±
                response = await self.responder.generate_final_response(superchat_state, source="superchat")
                
                # ì‘ë‹µì´ ìƒì„±ë˜ì—ˆë‹¤ë©´ TTS ë° ë¯¸ë””ì–´ ì²˜ë¦¬
                if response and response.get("assistant_text"):
                    print(f"âœ¨ Generated superchat response: {response['assistant_text']}")
                    
                    # MediaPacket ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° íë¡œ ì „ì†¡
                    if hasattr(self.responder, 'media_processor') and hasattr(self.responder, 'stream_session'):
                        try:
                            # í›„ì› ì‘ë‹µìš© ë¯¸ë””ì–´ íŒ¨í‚· ìƒì„±
                            media_packet = await self.responder.media_processor.create_media_packet(
                                ai_response=response["assistant_text"],
                                streamer_config={
                                    'streamer_id': self.streamer_id,
                                    'tts_engine': 'elevenlabs',
                                    'elevenlabs_voice': 'aneunjin',
                                    'elevenlabs_model': 'eleven_multilingual_v2'
                                },
                                context={
                                    'type': 'superchat_response',
                                    'user_id': msg.get('user_id'),
                                    'amount': msg.get('metadata', {}).get('amount', 0)
                                }
                            )
                            
                            if media_packet:
                                # ìŠ¤íŠ¸ë¦¬ë° ì„¸ì…˜ì— ë¯¸ë””ì–´ íŒ¨í‚· ì¶”ê°€
                                await self.responder.stream_session.enqueue_response(media_packet)
                                print(f"ğŸ¬ Superchat MediaPacket queued for streaming")
                            else:
                                print("âŒ Failed to create MediaPacket for superchat")
                                
                        except Exception as e:
                            print(f"âŒ Error creating superchat media: {e}")
                            import traceback
                            traceback.print_exc()
                    else:
                        print("âš ï¸ Media processor or stream session not available")
                else:
                    print("âš ï¸ No response generated for superchat")
                    
            except asyncio.CancelledError:
                print("Superchat worker task cancelled.")
                break
            except Exception as e:
                print(f"âŒ Error handling superchat: {e}")
                import traceback
                traceback.print_exc()
            finally:
                if 'msg' in locals() and self.superchat_q.empty():
                    self.superchat_q.task_done()

    async def broadcast_queue_state(self, room_id: str):
        """í ìƒíƒœë¥¼ í”„ë¡ íŠ¸ì—”ë“œì— ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        if not self.streamer_id or not room_id:
            return
            
        try:
            from channels.layers import get_channel_layer
            
            channel_layer = get_channel_layer()
            room_group_name = f'streaming_chat_{room_id}'
            
            # ì‹¤ì œ í ìƒíƒœ ì •ë³´ êµ¬ì„± (QueueManager + StreamSession í†µí•©)
            detailed_info = self._build_real_queue_info()
            
            # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” ìƒì„¸í•œ í ìƒíƒœ ì •ë³´ êµ¬ì„±
            queue_info = {
                'type': 'queue_debug_update',
                'detailed_queue_info': detailed_info if detailed_info else {
                    'request_queue': {
                        'pending_requests': [
                            {
                                'id': i,
                                'content': msg.get('content', '')[:50] + '...' if len(msg.get('content', '')) > 50 else msg.get('content', ''),
                                'user_id': msg.get('user_id', 'unknown'),
                                'topic': msg.get('topic', 'ì¼ë°˜'),
                                'salience': msg.get('salience', 0.0),
                                'timestamp': msg.get('ts', 0)
                            }
                            for i, msg in enumerate(list(self.queue.general_queue)[-5:])  # ìµœê·¼ 5ê°œë§Œ
                        ],
                        'current_processing': None,
                        'total_pending': len(self.queue.general_queue)
                    },
                    'response_queue': {
                        'pending_responses': [],
                        'current_playing': None,
                        'total_pending': self.superchat_q.qsize()
                    },
                    'recent_history': [],
                    'metrics': {
                        'total_processed': len(self.queue.general_queue),
                        'success_rate': 0.95,
                        'avg_response_time': 2.3,
                        'current_load': min(1.0, len(self.queue.general_queue) / 10.0)
                    },
                    'current_seq': 0
                },
                'session_info': {
                    'queue_length': len(self.queue.general_queue),
                    'superchat_queue_length': self.superchat_q.qsize(),
                    'current_topic': self.topic.topic_ctx.get("active_label", "ì—†ìŒ"),
                    'is_processing': self.queue.is_busy(),
                    'agent_status': 'active'
                },
                'queue_status': {
                    'lastProcessedSeq': detailed_info.get('current_seq', 0),
                    'connectionStatus': 'connected',
                    'lastUpdate': datetime.now().isoformat()
                },
                'timestamp': datetime.now().isoformat()
            }
            
            await channel_layer.group_send(
                room_group_name,
                queue_info
            )
        except Exception as e:
            print(f"í ìƒíƒœ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    def _build_real_queue_info(self):
        """ì‹¤ì œ í ìƒíƒœ ì •ë³´ êµ¬ì„± (QueueManager + StreamSession í†µí•©)"""
        from datetime import datetime
        import time
        
        # QueueManagerì˜ ì‹¤ì œ general_queue ì •ë³´
        pending_requests = []
        with self.queue._q_lock:
            for index, msg in enumerate(list(self.queue.general_queue)[-10:]):  # ìµœê·¼ 10ê°œ
                pending_requests.append({
                    "position": index + 1,
                    "message": msg.get('content', '')[:50],
                    "username": msg.get('user_id', 'unknown'),
                    "user_id": msg.get('user_id'),
                    "topic": msg.get('topic', 'ì¼ë°˜'),
                    "salience": msg.get('salience', 0.0),
                    "timestamp": msg.get('ts', time.monotonic()),
                    "waiting_time": max(0, time.monotonic() - msg.get('ts', time.monotonic()))
                })
        
        # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ì •ë³´
        current_processing = None
        if self.queue.is_busy():
            current_processing = {
                "status": "processing",
                "topic": self.topic.topic_ctx.get("active_label", "ì¼ë°˜"),
                "started_at": time.monotonic()
            }
        
        # StreamSession Response Queue ì •ë³´
        response_queue_info = {"length": 0, "pending_packets": []}
        if hasattr(self.responder, 'stream_session') and self.responder.stream_session:
            session = self.responder.stream_session
            response_queue_info["length"] = session.response_queue.qsize()
            
        # Superchat Queue ì •ë³´
        superchat_queue_length = self.superchat_q.qsize()
        
        return {
            "session_id": getattr(self.responder, 'stream_session', {}).session_id if hasattr(self.responder, 'stream_session') else "unknown",
            "timestamp": int(time.time() * 1000),
            
            # Request Queue ìƒíƒœ (ì‹¤ì œ QueueManager ë°ì´í„°)
            "request_queue": {
                "length": len(self.queue.general_queue),
                "is_processing": self.queue.is_busy(),
                "current_processing": current_processing,
                "pending_requests": pending_requests,
            },
            
            # Response Queue ìƒíƒœ (ì‹¤ì œ StreamSession ë°ì´í„°)
            "response_queue": {
                "length": response_queue_info["length"],
                "is_playing": False,  # í˜„ì¬ ì¬ìƒ ìƒíƒœëŠ” í´ë¼ì´ì–¸íŠ¸ì—ì„œ ê´€ë¦¬
                "current_playing": None,
                "pending_packets": response_queue_info["pending_packets"],
                "total_played": 0
            },
            
            # Superchat Queue ìƒíƒœ
            "superchat_queue": {
                "length": superchat_queue_length,
                "pending_superchats": []
            },
            
            # ê¸°ë³¸ ìƒíƒœ ì •ë³´
            "queue_length": len(self.queue.general_queue),
            "is_processing": self.queue.is_busy(),
            "current_seq": getattr(getattr(self.responder, 'stream_session', None), 'seq', 0),
            "uptime_ms": int(time.time() * 1000),
            
            # ë©”íŠ¸ë¦­
            "metrics": {
                "total_processed": len(self.queue.general_queue),
                "success_rate": 0.95,
                "avg_response_time": 3.2,
                "current_load": min(1.0, len(self.queue.general_queue) / 10.0),
                "current_topic": self.topic.topic_ctx.get("active_label", "ì—†ìŒ")
            },
            
            # ìµœê·¼ ì²˜ë¦¬ ì´ë ¥ (ì„ì‹œ)
            "recent_history": [
                {
                    "timestamp": int(time.time() * 1000) - (i * 30000),
                    "message": f"ì²˜ë¦¬ëœ ë©”ì‹œì§€ {i+1}",
                    "username": "user_example",
                    "status": "completed",
                    "processing_time": 2.5 + (i * 0.3)
                }
                for i in range(min(3, len(self.queue.general_queue)))
            ]
        }
