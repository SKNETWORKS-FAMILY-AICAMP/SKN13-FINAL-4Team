# backend/chat/agent/idle.py
import asyncio
import time
import uuid
from datetime import datetime
from typing import Callable
from .queue_manager import QueueManager
from .story import StoryRepository, Story
from .responder import Responder
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# ìˆœí™˜ ì°¸ì¡°ë¥¼ í”¼í•˜ê¸° ìœ„í•´ íƒ€ì… íŒíŠ¸ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from .agent import LoveStreamerAgent

class IdleManager:
    """ë¬´ì±„íŒ… ìë™ ë©˜íŠ¸/ë¦¬ìº¡ + ì •ì²´ êµ¬ì œ + í”„ë¦¬ì•„ì´ë“¤"""
    def __init__(self, agent: 'LoveStreamerAgent', llm: ChatOpenAI, queue_mgr: QueueManager, story_repo: StoryRepository, responder: Responder, streamer_id: str = None):
        self.agent = agent
        self.llm = llm
        self.queue_mgr = queue_mgr
        self.story_repo = story_repo
        self.responder = responder
        self.streamer_id = streamer_id
        
        self.inference_client = None
        if streamer_id:
            try:
                from ..services.inference_client import InferenceClient
                self.inference_client = InferenceClient(streamer_id)
            except ImportError:
                pass
        self.IDLE_RECAP_COOLDOWN_SEC = 120
        self._last_idle_recap_ts = 0.0
        self.FORCE_GRAPH_RUN_IF_STALE_SEC = 10
        self._last_graph_trigger_ts = 0.0
        self.PREIDLE_MIN_QUIET_SEC = 6
        self.PREIDLE_COOLDOWN_SEC = 45
        self._last_preidle_ts = 0.0
        self.STORY_CHUNK_CHARS = 180
        self.STORY_CHUNK_DELAY = 1.2
        self.START_WITH_STORY = True
        self._did_start_with_story = False
        self.trigger_graph = lambda reason: None
        self.topic_label = lambda: ""
        self.bootstrap_topic_from_tail = lambda: None

    def reset_cooldown(self):
        print("[IdleManager] â° ì¿¨ë‹¤ìš´ íƒ€ì´ë¨¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        self._last_idle_recap_ts = time.time()

    def mark_graph_trigger(self):
        self._last_graph_trigger_ts = time.time()

    async def submit_story(self, title: str, body: str, user_id: str = "guest", submitted_at: str = None) -> str:
        sid = str(uuid.uuid4())
        await self.story_repo.add(Story(story_id=sid, user_id=user_id, title=(title or "").strip(), body=(body or "").strip(), submitted_at=submitted_at or datetime.now().strftime("%Y-%m-%d %H:%M:%S"), status="pending"))
        return sid

    async def _open_topic_based_dialogue(self) -> str:
        print("[IdleManager] ğŸ—£ï¸ _open_topic_based_dialogue: ì£¼ì œ ê¸°ë°˜ ëŒ€í™” ì—´ê¸° ì‹œë„.")
        if (time.time() - self._last_idle_recap_ts) < self.IDLE_RECAP_COOLDOWN_SEC:
            print(f"[IdleManager] ì¿¨ë‹¤ìš´ ì¤‘. ë‚¨ì€ ì‹œê°„: {self.IDLE_RECAP_COOLDOWN_SEC - (time.time() - self._last_idle_recap_ts):.1f}ì´ˆ")
            return ""

        active_topic = self.topic_label() or "ì—°ì•  ê³ ë¯¼"
        print(f"[IdleManager] í˜„ì¬ í™œì„± ì£¼ì œ: '{active_topic}'")

        # --- ë¦¬íŒ©í† ë§ëœ ë¶€ë¶„ ---
        # 1. Responderë¡œë¶€í„° ì™„ì „í•œ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        base_persona_prompt = self.responder._format_persona_prompt()
        
        # 2. ììœ¨ ë°œí™” ìƒí™©ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì²´í™”í•©ë‹ˆë‹¤.
        situation_prompt = (
            f"\n# í˜„ì¬ ìƒí™©: ììœ¨ ë°œí™”\n"
            f"í˜„ì¬ ì±„íŒ…ì´ ì ì‹œ ì—†ëŠ” ìƒíƒœì•¼. '{active_topic}' ì£¼ì œì™€ ê´€ë ¨í•´ì„œ ì‹œì²­ìë“¤ì˜ ì°¸ì—¬ë¥¼ ìœ ë„í•  ìˆ˜ ìˆëŠ” "
            f"í¥ë¯¸ë¡œìš´ í˜¼ì£ë§ì´ë‚˜ ê°œë°©í˜• ì§ˆë¬¸ì„ ë„ˆì˜ í˜ë¥´ì†Œë‚˜ì— ë§ê²Œ ì™„ë²½í•˜ê²Œ ì—°ê¸°í•´ì„œ ìƒì„±í•´ì¤˜. "
            f"ì´ ë°œì–¸ì€ ë°©ì†¡ì˜ ë‹¤ìŒ íë¦„ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ë©˜íŠ¸ê°€ ë  ê±°ì•¼. (2~3 ë¬¸ì¥ ë‚´ì™¸ë¡œ)"
        )
        
        # 3. í˜ë¥´ì†Œë‚˜ì™€ ìƒí™© í”„ë¡¬í”„íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ ì™„ì„±í•©ë‹ˆë‹¤.
        sys_prompt = f"{base_persona_prompt}{situation_prompt}"
        user_prompt = "ì, ë°©ì†¡ì„ ì´ëŒì–´ê°ˆ ë©‹ì§„ ë©˜íŠ¸ë¥¼ ì‹œì‘í•´ì¤˜!"

        try:
            text = None
            if self.inference_client:
                try:
                    # 4. ë‹¨ì¼ LLM í˜¸ì¶œë¡œ ìµœì¢… í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
                    text = await self.inference_client.generate_text(system_prompt=sys_prompt, user_prompt=user_prompt)
                    print(f"[IdleManager] âœ… [Single Call] ì¶”ë¡  ì„œë²„ë¡œ ìµœì¢… ììœ¨ ë°œí™” ìƒì„± ì„±ê³µ.")
                except Exception as e:
                    print(f"[IdleManager] âš ï¸ ì¶”ë¡  ì„œë²„ í˜¸ì¶œ ì‹¤íŒ¨: {e}, OpenAIë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            
            if text is None:
                res = await self.llm.ainvoke([SystemMessage(content=sys_prompt), HumanMessage(content=user_prompt)])
                text = getattr(res, "content", str(res)).strip()
                print(f"[IdleManager] âœ… [Single Call] OpenAIë¡œ ìµœì¢… ììœ¨ ë°œí™” ìƒì„± ì„±ê³µ.")

            if not text:
                print("[IdleManager] âŒ LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í•˜ì—¬ ììœ¨í–‰ë™ ì‹¤íŒ¨.")
                return ""
            
            self._last_idle_recap_ts = time.time()
            print(f"[IdleManager] ìƒì„±ëœ ìµœì¢… ììœ¨í–‰ë™ ë©”ì‹œì§€: '{text}'")
            return text.strip()
        except Exception as e:
            print(f"[IdleManager] âŒ LLM í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return ""

    async def _play_story_readout(self, story: Story, is_resume: bool = False):
        """ì‚¬ì—°ì„ LLMì— ì „ë‹¬í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•˜ê³ , ì™„ë£Œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        print(f"!!! DEBUG: _play_story_readout ì§„ì…. Story ID: {story.story_id}")

        story_content = f"ì œëª©: {story.title}\n\në‚´ìš©: {story.body}"
        story_state = {
            "messages": [HumanMessage(content=story_content)],
            "type": "story",
            "categories": ["ì‚¬ì—°ì½ê¸°", "ê³µê°"],
            "best_chat": story_content,
            "user_id": story.user_id,
            "chat_date": story.submitted_at,
            "db_greeting_info": {"exists": False},
            "__no_selection": False,
            "assistant_emotion": "empathetic",
            "msg_id": f"story-{story.story_id}"
        }

        asyncio.create_task(self.responder.generate_final_response(story_state))
        print(f"!!! DEBUG: Responderì— ì‚¬ì—° ì²˜ë¦¬ ì‘ì—… ì „ë‹¬ ì™„ë£Œ. Story ID: {story.story_id}")

        await self.story_repo.save_resume(story.story_id, "")
        await self.story_repo.mark_done(story.story_id)
        print(f"!!! DEBUG: Story ìƒíƒœë¥¼ 'done'ìœ¼ë¡œ ë³€ê²½ ì™„ë£Œ. Story ID: {story.story_id}")

        await asyncio.sleep(self.STORY_CHUNK_DELAY)

    async def _resume_or_new_or_recap(self, interval: int, *, force_story: bool = False, story_only: bool = False) -> bool:
        remaining = await self.story_repo.get_resume()
        if remaining:
            fake = Story(story_id=f"resume-{uuid.uuid4()}", user_id="system", title="", body=remaining, submitted_at=datetime.now().strftime("%Y-%m-%d %H:%M:%S"), status="reading")
            await self._play_story_readout(fake, is_resume=True)
            self._last_preidle_ts = time.time()
            return True
        has = await self.story_repo.has_pending()
        if (force_story and has) or (has and (time.time() - self._last_preidle_ts) >= self.PREIDLE_COOLDOWN_SEC):
            story = await self.story_repo.pop_next()
            if story:
                print(f"!!! DEBUG: Story popped from DB, starting readout. Story ID: {story.story_id}") # ë””ë²„ê¹… ë¡œê·¸
                await self._play_story_readout(story)
                self._last_preidle_ts = time.time()
                return True
        if story_only: return False
        msg = await self._open_topic_based_dialogue()
        if msg:
            self.queue_mgr.mark_event()
            autonomous_state = {
                "messages": [HumanMessage(content=msg)],
                "type": "normal",
                "categories": ["ììœ¨í–‰ë™", self.topic_label() or "ì¼ë°˜"],
                "best_chat": msg,
                "user_id": "system_idle",
                "chat_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "db_greeting_info": {"exists": False},
                "__no_selection": False,
                "assistant_emotion": "neutral",
                "msg_id": f"idle-{uuid.uuid4()}",
                "skip_llm_generation": True  # --- ë¦¬íŒ©í† ë§ëœ ë¶€ë¶„ ---
            }
            asyncio.create_task(self.responder.generate_final_response(autonomous_state))
            await asyncio.sleep(max(5, interval // 3))
            return True
        return False

    async def idle_loop(self, interval: int = 15):
        while True:
            await asyncio.sleep(2)
            try:
                busy = self.queue_mgr.is_busy()
                has_work = self.queue_mgr.has_active_work()
                quiet = (time.time() - self.queue_mgr.last_event_ts) >= min(interval, self.PREIDLE_MIN_QUIET_SEC)
                if self.START_WITH_STORY and not self._did_start_with_story:
                    if not busy and not has_work:
                        if await self._resume_or_new_or_recap(interval, force_story=True, story_only=True): continue
                    self._did_start_with_story = True
                if quiet and not busy and not has_work:
                    if await self._resume_or_new_or_recap(interval): continue
                stale = quiet and not busy and has_work and ((time.time() - self._last_graph_trigger_ts) >= self.FORCE_GRAPH_RUN_IF_STALE_SEC)
                if stale:
                    if not self.topic_label(): self.bootstrap_topic_from_tail()
                    self.trigger_graph("stale_work_fallback")
                    await asyncio.sleep(max(5, interval // 3))
            except Exception: pass

    def set_graph_trigger(self, trigger_cb: Callable[[str], None]):
        self.trigger_graph = trigger_cb

    def set_bootstrap_helpers(self, topic_label_fn: Callable[[], str], bootstrap_fn: Callable[[], None]):
        self.topic_label = topic_label_fn
        self.bootstrap_topic_from_tail = bootstrap_fn
