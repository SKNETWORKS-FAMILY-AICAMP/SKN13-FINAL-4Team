"""
AI ì¸í”Œë£¨ì–¸ì„œ ë¯¸ë””ì–´ ì²˜ë¦¬ í—ˆë¸Œ
í…ìŠ¤íŠ¸, TTS, ë¹„ë””ì˜¤ë¥¼ í†µí•© ì²˜ë¦¬í•˜ì—¬ ë™ê¸°í™”ëœ ë¸Œë¡œë“œìºìŠ¤íŒ… íŒ¨í‚·ì„ ìƒì„±
DDD ì•„í‚¤í…ì²˜ ê¸°ë°˜ StreamSession ì‚¬ìš©
"""
import time
import uuid
import asyncio
import logging
import base64
from typing import Dict, Any, Optional, Tuple
from datetime import datetime
from django.conf import settings
from django.core.cache import cache
import openai
import requests
from .video_manager import VideoSelector
from .streaming.domain.stream_session import StreamSession, MediaTrack, MediaPacket

logger = logging.getLogger(__name__)

class MediaProcessingHub:
    """ë¯¸ë””ì–´ ì²˜ë¦¬ í—ˆë¸Œ - TTS, ë¹„ë””ì˜¤, ìë§‰ì„ í†µí•© ì²˜ë¦¬ (DDD ê¸°ë°˜ StreamSession ì‚¬ìš©)"""
    
    def __init__(self):
        self.video_selector = VideoSelector()
        self.processing_cache = {}  # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€ìš© ìºì‹œ
        self.sessions: Dict[str, StreamSession] = {}  # ë£¸ë³„ ì„¸ì…˜ ê´€ë¦¬
        
    async def process_ai_response(self, text: str, streamer_config: Dict, room_name: str, emotion: str = 'neutral') -> Dict[str, Any]:
        """
        AI ì‘ë‹µì„ í†µí•© ì²˜ë¦¬í•˜ì—¬ ë™ê¸°í™”ëœ ë¯¸ë””ì–´ íŒ¨í‚· ìƒì„± (ë‹¨ìˆœí™”ëœ ë²„ì „)
        
        Args:
            text: AI ì‘ë‹µ í…ìŠ¤íŠ¸
            streamer_config: ìŠ¤íŠ¸ë¦¬ë¨¸ ì„¤ì • (ìŒì„± ë“±)
            room_name: ë°©ì†¡ ë£¸ ì´ë¦„
            emotion: LLMì—ì„œ ì œê³µí•œ ê°ì • (ê¸°ë³¸ê°’: neutral)
            
        Returns:
            ë™ê¸°í™”ëœ ë¯¸ë””ì–´ íŒ¨í‚·
        """
        try:
            process_start = time.time()
            sync_id = uuid.uuid4().hex
            
            logger.info(f"ğŸ¬ ë¯¸ë””ì–´ ì²˜ë¦¬ ì‹œì‘: {sync_id[:8]} - {text[:50]}... (ê°ì •: {emotion})")
            
            # 1. ìºì‹œ í™•ì¸ (ë””ë²„ê¹…ì„ ìœ„í•´ ì„ì‹œ ë¹„í™œì„±í™”)
            cache_key = f"media_process_{hash(text)}_{emotion}"
            # cached_result = cache.get(cache_key)
            # if cached_result:
            #     logger.info(f"ğŸ“¦ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©: {sync_id[:8]}")
            #     cached_result['sync_id'] = sync_id
            #     return cached_result
            
            # 2. TTS ìƒì„±
            tts_result = await self._generate_tts_async(text, streamer_config)
            
            # 3. ë¹„ë””ì˜¤ í´ë¦½ ì„ íƒ (talk ë¹„ë””ì˜¤) - ìºë¦­í„° ID ê¸°ë°˜
            character_id = streamer_config.get('character_id', 'jammin-i')  # ê¸°ë³¸ê°’ ì„¤ì •
            talk_video = self.video_selector.get_talk_video(emotion, character_id)
            idle_video = self.video_selector.get_idle_video(emotion, character_id)
            
            # 4. ìë§‰ íƒ€ì´ë° ìƒì„±
            subtitle_timing = self._generate_subtitle_timing(text, tts_result['duration'])
            
            # 5. ë™ê¸°í™” íŒ¨í‚· ìƒì„±
            sync_packet = {
                'sync_id': sync_id,
                'timestamp': time.time(),
                'room_name': room_name,
                'content': {
                    'text': text,
                    'audio_url': tts_result['audio_url'],
                    'audio_duration': tts_result['duration'],
                    'talk_video': talk_video,
                    'idle_video': idle_video,
                    'emotion': emotion,
                    'subtitle_timing': subtitle_timing,
                    'tts_info': tts_result.get('tts_info', {}),
                    'video_sequence': {
                        'start_state': 'talk',  # TTS ì‹œì‘ ì‹œ talk ë¹„ë””ì˜¤
                        'end_state': 'idle',    # TTS ì™„ë£Œ í›„ idle ë¹„ë””ì˜¤
                        'talk_duration': tts_result['duration']
                    }
                },
                'sync_timing': {
                    'broadcast_delay': 0.5,  # 500ms ë²„í¼ë§
                    'processing_time': time.time() - process_start,
                    'scheduled_start': time.time() + 0.5,
                    'idle_return_delay': tts_result['duration'] + 1.0  # TTS ì™„ë£Œ 1ì´ˆ í›„ idle ë³µê·€
                },
                'metadata': {
                    'streamer_id': streamer_config.get('streamer_id'),
                    'voice_settings': streamer_config.get('voice_settings', {}),
                    'created_at': datetime.now().isoformat(),
                    'system_version': 'simple_idle_talk'
                }
            }
            
            # 6. ê²°ê³¼ ìºì‹± (10ë¶„ê°„)
            cache.set(cache_key, sync_packet, 600)
            
            logger.info(f"âœ… ë¯¸ë””ì–´ ì²˜ë¦¬ ì™„ë£Œ: {sync_id[:8]} ({sync_packet['sync_timing']['processing_time']:.2f}s)")
            logger.info(f"   Talk ë¹„ë””ì˜¤: {talk_video}, Idle ë³µê·€: {idle_video}")
            
            return sync_packet
            
        except Exception as e:
            logger.error(f"âŒ ë¯¸ë””ì–´ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return self._create_error_packet(text, str(e), room_name)
    
    
    async def _generate_tts_async(self, text: str, streamer_config: Dict) -> Dict[str, Any]:
        """TTS ìƒì„± (ë¹„ë™ê¸°)"""
        logger.info(f"ğŸ”Š TTS ìƒì„± ì‹œì‘: {text[:30]}...")
        voice_settings = streamer_config.get('voice_settings', {})
        engine = voice_settings.get('engine', 'elevenlabs')
        logger.info(f"ğŸ”Š TTS ì—”ì§„: {engine}, ì„¤ì •: {voice_settings}")
        
        if engine == 'elevenlabs':
            return await self._generate_elevenlabs_tts(text, voice_settings)
        elif engine == 'openai':
            return await self._generate_openai_tts(text, voice_settings)
        else:
            # ê¸°ë³¸ê°’: ElevenLabs
            return await self._generate_elevenlabs_tts(text, voice_settings)
    
    async def _generate_elevenlabs_tts(self, text: str, voice_settings: Dict) -> Dict[str, Any]:
        """ElevenLabs TTS ìƒì„±"""
        try:
            logger.info(f"ğŸ¤ ElevenLabs TTS í˜¸ì¶œ ì‹œì‘: {text[:30]}...")
            api_key = settings.ELEVENLABS_API_KEY
            if not api_key:
                logger.error("âŒ ElevenLabs API key not configured")
                raise ValueError("ElevenLabs API key not configured")
            
            # í”„ë¡ íŠ¸ì—”ë“œ ì„¤ì • í‚¤ ë§¤í•‘ ì²˜ë¦¬
            voice_id = voice_settings.get('voice_id') or voice_settings.get('elevenLabsVoice', 'aneunjin')
            logger.info(f"ğŸµ ì„ íƒëœ ìŒì„±: {voice_id}")
            
            # ìŒì„± ì„¤ì • ë§¤í•‘ (tts_elevenlabs_service.pyì™€ ë™ì¼í•˜ê²Œ í†µì¼)
            voice_map = {
                'kimtaeri': '6ZND2SlfJqI0OOEHe2by',    # ê¹€íƒœë¦¬ (í•œêµ­ ì—¬ì„± ë°°ìš°)
                'kimminjeong': 'eTiuJAsb9mqCyH5gFsS9', # ê¹€ë¯¼ì • (í•œêµ­ ì—¬ì„± ë°°ìš°)  
                'jinseonkyu': 'pWPHfY5KntyWbx2FxSb7', # ì§„ì„ ê·œ (í•œêµ­ ë‚¨ì„± ë°°ìš°)
                'parkchangwook': 'RQVmMEdMMcmOuv6Fz268', # ë°•ì°½ìš± (í•œêµ­ ë‚¨ì„± ë°°ìš°)
                'aneunjin': 'pRxVZ0v1oH2CqQJWHAty',  # ì•ˆì€ì§„ (í•œêµ­ ì—¬ì„± ë°°ìš°)
                'jiyoung': 'AW5wrnG1jVizOYY7R1Oo'     # JiYoung (í™œê¸°ì°¬ ì Šì€ ì—¬ì„± ìŒì„±) - ì˜¬ë°”ë¥¸ ì—¬ì„± Voice ID
            }
            
            actual_voice_id = voice_map.get(voice_id, voice_map['aneunjin'])
            
            url = f"https://api.elevenlabs.io/v1/text-to-speech/{actual_voice_id}"
            
            headers = {
                "Accept": "audio/mpeg",
                "Content-Type": "application/json",
                "xi-api-key": api_key
            }
            
            # í”„ë¡ íŠ¸ì—”ë“œ ì„¤ì • í‚¤ ë§¤í•‘
            model_id = voice_settings.get('model_id') or voice_settings.get('elevenLabsModel', 'eleven_multilingual_v2')
            stability = voice_settings.get('stability') or voice_settings.get('elevenLabsStability', 0.75)
            similarity = voice_settings.get('similarity_boost') or voice_settings.get('elevenLabsSimilarity', 0.85)
            style = voice_settings.get('style') or voice_settings.get('elevenLabsStyle', 0.0)
            speaker_boost = voice_settings.get('use_speaker_boost') 
            if speaker_boost is None:
                speaker_boost = voice_settings.get('elevenLabsSpeakerBoost', True)
            
            logger.info(f"ğŸ›ï¸ TTS íŒŒë¼ë¯¸í„°: model={model_id}, stability={stability}, similarity={similarity}")
            
            data = {
                "text": text,
                "model_id": model_id,
                "voice_settings": {
                    "stability": stability,
                    "similarity_boost": similarity,
                    "style": style,
                    "use_speaker_boost": speaker_boost
                }
            }
            
            # ë¹„ë™ê¸° HTTP ìš”ì²­
            response = await asyncio.to_thread(requests.post, url, json=data, headers=headers)
            
            if response.status_code != 200:
                logger.error(f"âŒ ElevenLabs API ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                raise Exception(f"ElevenLabs API error: {response.status_code}")
            
            logger.info(f"âœ… ElevenLabs API ì„±ê³µ: {len(response.content)} bytes")
            
            # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ base64ë¡œ ì¸ì½”ë”©í•´ì„œ ì§ì ‘ ì „ë‹¬
            audio_base64 = base64.b64encode(response.content).decode('utf-8')
            audio_data_url = f"data:audio/mpeg;base64,{audio_base64}"
            logger.info(f"ğŸµ ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(response.content)} bytes -> base64")
            
            # ì˜¤ë””ì˜¤ ê¸¸ì´ ì¶”ì • (ì‹¤ì œë¡œëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„ í•„ìš”)
            estimated_duration = len(text) * 0.1  # ëŒ€ëµì  ì¶”ì •
            logger.info(f"â±ï¸ ì¶”ì • ì˜¤ë””ì˜¤ ê¸¸ì´: {estimated_duration}ì´ˆ")
            
            return {
                'audio_url': audio_data_url,
                'duration': estimated_duration,
                'tts_info': {
                    'engine': 'elevenlabs',
                    'voice': voice_id,
                    'model': data['model_id'],
                    'file_size': len(response.content),
                    'format': 'base64_data_url'
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ElevenLabs TTS ì‹¤íŒ¨: {str(e)}")
            # OpenAI í´ë°±
            return await self._generate_openai_tts(text, voice_settings)
    
    async def _generate_openai_tts(self, text: str, voice_settings: Dict) -> Dict[str, Any]:
        """OpenAI TTS ìƒì„± (í´ë°±)"""
        try:
            client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
            
            response = await client.audio.speech.create(
                model="tts-1-hd",
                voice=voice_settings.get('openai_voice', 'alloy'),
                input=text,
                speed=voice_settings.get('speed', 1.0)
            )
            
            # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ base64ë¡œ ì¸ì½”ë”©í•´ì„œ ì§ì ‘ ì „ë‹¬
            audio_base64 = base64.b64encode(response.content).decode('utf-8')
            audio_data_url = f"data:audio/mpeg;base64,{audio_base64}"
            logger.info(f"ğŸµ OpenAI ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(response.content)} bytes -> base64")
            
            estimated_duration = len(text) * 0.08
            
            return {
                'audio_url': audio_data_url,
                'duration': estimated_duration,
                'tts_info': {
                    'engine': 'openai',
                    'voice': voice_settings.get('openai_voice', 'alloy'),
                    'model': 'tts-1-hd',
                    'fallback_used': True,
                    'format': 'base64_data_url'
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ OpenAI TTS í´ë°± ì‹¤íŒ¨: {str(e)}")
            return self._create_tts_error_result(text, str(e))
    
    
    def _generate_subtitle_timing(self, text: str, duration: float) -> Dict[str, Any]:
        """ìë§‰ íƒ€ì´ë° ìƒì„±"""
        words = text.split()
        time_per_word = duration / len(words) if words else 0
        
        subtitle_segments = []
        current_time = 0
        
        for i, word in enumerate(words):
            segment_duration = time_per_word
            subtitle_segments.append({
                'word': word,
                'start': current_time,
                'end': current_time + segment_duration,
                'index': i
            })
            current_time += segment_duration
        
        return {
            'segments': subtitle_segments,
            'total_duration': duration,
            'words_count': len(words)
        }
    
    
    def _create_error_packet(self, text: str, error: str, room_name: str) -> Dict[str, Any]:
        """ì—ëŸ¬ íŒ¨í‚· ìƒì„±"""
        return {
            'sync_id': uuid.uuid4().hex,
            'timestamp': time.time(),
            'room_name': room_name,
            'content': {
                'text': text,
                'error': error,
                'fallback_mode': True
            },
            'sync_timing': {
                'broadcast_delay': 0,
                'processing_time': 0,
                'scheduled_start': time.time()
            },
            'metadata': {
                'error': True,
                'created_at': datetime.now().isoformat()
            }
        }
    
    def _create_tts_error_result(self, text: str, error: str) -> Dict[str, Any]:
        """TTS ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            'audio_url': '',
            'duration': 0,
            'tts_info': {
                'engine': 'none',
                'error': error,
                'fallback_failed': True
            }
        }