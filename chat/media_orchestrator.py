"""
AI ì¸í”Œë£¨ì–¸ì„œ ë¯¸ë””ì–´ ì²˜ë¦¬ í—ˆë¸Œ
í…ìŠ¤íŠ¸, TTS, ë¹„ë””ì˜¤ë¥¼ í†µí•© ì²˜ë¦¬í•˜ì—¬ ë™ê¸°í™”ëœ ë¸Œë¡œë“œìºìŠ¤íŒ… íŒ¨í‚·ì„ ìƒì„±
DDD ì•„í‚¤í…ì²˜ ê¸°ë°˜ StreamSession ì‚¬ìš©
"""
import time
import uuid
import asyncio
import logging
import base64
import json
from typing import Dict, Any, Optional, Tuple, List
from datetime import datetime
from django.conf import settings
from django.core.cache import cache
import openai
import requests
from .video_manager import VideoSelector
from .streaming.domain.stream_session import StreamSession, MediaTrack, MediaPacket

logger = logging.getLogger(__name__)

class MediaProcessingHub:
    """ë¯¸ë””ì–´ ì²˜ë¦¬ í—ˆë¸Œ - TTS, ë¹„ë””ì˜¤, ìë§‰ì„ í†µí•© ì²˜ë¦¬ (DDD ê¸°ë°˜ StreamSession ì‚¬ìš©)"""
    
    def __init__(self):
        self.video_selector = VideoSelector()
        self.processing_cache = {}  # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€ìš© ìºì‹œ
        self.sessions: Dict[str, StreamSession] = {}  # ë£¸ë³„ ì„¸ì…˜ ê´€ë¦¬
        
    async def process_ai_response(self, text: str, streamer_config: Dict, room_name: str, emotion: str = 'neutral') -> Dict[str, Any]:
        """
        AI ì‘ë‹µì„ í†µí•© ì²˜ë¦¬í•˜ì—¬ ë™ê¸°í™”ëœ ë¯¸ë””ì–´ íŒ¨í‚· ìƒì„± (ë‹¨ìˆœí™”ëœ ë²„ì „)
        
        Args:
            text: AI ì‘ë‹µ í…ìŠ¤íŠ¸
            streamer_config: ìŠ¤íŠ¸ë¦¬ë¨¸ ì„¤ì • (ìŒì„± ë“±)
            room_name: ë°©ì†¡ ë£¸ ì´ë¦„
            emotion: LLMì—ì„œ ì œê³µí•œ ê°ì • (ê¸°ë³¸ê°’: neutral)
            
        Returns:
            ë™ê¸°í™”ëœ ë¯¸ë””ì–´ íŒ¨í‚·
        """
        try:
            process_start = time.time()
            sync_id = uuid.uuid4().hex
            
            logger.info(f"ğŸ¬ ë¯¸ë””ì–´ ì²˜ë¦¬ ì‹œì‘: {sync_id[:8]} - {text[:50]}... (ê°ì •: {emotion})")
            
            # 1. ìºì‹œ í™•ì¸ (ë””ë²„ê¹…ì„ ìœ„í•´ ì„ì‹œ ë¹„í™œì„±í™”)
            cache_key = f"media_process_{hash(text)}_{emotion}"
            # cached_result = cache.get(cache_key)
            # if cached_result:
            #     logger.info(f"ğŸ“¦ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©: {sync_id[:8]}")
            #     cached_result['sync_id'] = sync_id
            #     return cached_result
            
            # 2. TTS ìƒì„±
            tts_result = await self._generate_tts_async(text, streamer_config)
            
            # 3. ë¹„ë””ì˜¤ í´ë¦½ ì„ íƒ (talk ë¹„ë””ì˜¤) - ìºë¦­í„° ID ê¸°ë°˜
            character_id = streamer_config.get('character_id', 'jammin-i')  # ê¸°ë³¸ê°’ ì„¤ì •
            talk_video = self.video_selector.get_talk_video(emotion, character_id)
            idle_video = self.video_selector.get_idle_video(emotion, character_id)
            
            # 4. ìë§‰ íƒ€ì´ë° ìƒì„±
            subtitle_timing = self._generate_subtitle_timing(text, tts_result['duration'])
            
            # 5. ë™ê¸°í™” íŒ¨í‚· ìƒì„±
            sync_packet = {
                'sync_id': sync_id,
                'timestamp': time.time(),
                'room_name': room_name,
                'content': {
                    'text': text,
                    'audio_url': tts_result['audio_url'],
                    'audio_duration': tts_result['duration'],
                    'talk_video': talk_video,
                    'idle_video': idle_video,
                    'emotion': emotion,
                    'subtitle_timing': subtitle_timing,
                    'tts_info': tts_result.get('tts_info', {}),
                    'video_sequence': {
                        'start_state': 'talk',  # TTS ì‹œì‘ ì‹œ talk ë¹„ë””ì˜¤
                        'end_state': 'idle',    # TTS ì™„ë£Œ í›„ idle ë¹„ë””ì˜¤
                        'talk_duration': tts_result['duration']
                    }
                },
                'sync_timing': {
                    'broadcast_delay': 0.5,  # 500ms ë²„í¼ë§
                    'processing_time': time.time() - process_start,
                    'scheduled_start': time.time() + 0.5,
                    'idle_return_delay': tts_result['duration'] + 1.0  # TTS ì™„ë£Œ 1ì´ˆ í›„ idle ë³µê·€
                },
                'metadata': {
                    'streamer_id': streamer_config.get('streamer_id'),
                    'voice_settings': streamer_config.get('voice_settings', {}),
                    'created_at': datetime.now().isoformat(),
                    'system_version': 'simple_idle_talk'
                }
            }
            
            # 6. ê²°ê³¼ ìºì‹± (10ë¶„ê°„)
            cache.set(cache_key, sync_packet, 600)
            
            logger.info(f"âœ… ë¯¸ë””ì–´ ì²˜ë¦¬ ì™„ë£Œ: {sync_id[:8]} ({sync_packet['sync_timing']['processing_time']:.2f}s)")
            logger.info(f"   Talk ë¹„ë””ì˜¤: {talk_video}, Idle ë³µê·€: {idle_video}")
            
            return sync_packet
            
        except Exception as e:
            logger.error(f"âŒ ë¯¸ë””ì–´ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return self._create_error_packet(text, str(e), room_name)
    
    async def generate_tracks_with_cancellation(self, request_data: Dict, cancel_event: 'asyncio.Event') -> Optional[List]:
        """
        ì·¨ì†Œ ê°€ëŠ¥í•œ MediaTrack ìƒì„± (StreamSession Queue ì‹œìŠ¤í…œ ìš©)
        
        Args:
            request_data: ìš”ì²­ ë°ì´í„° (message, streamer_config ë“±)
            cancel_event: ì·¨ì†Œ ì´ë²¤íŠ¸ (setë˜ë©´ ì‘ì—… ì¤‘ë‹¨)
            
        Returns:
            List[MediaTrack] or None: ìƒì„±ëœ íŠ¸ë™ë“¤ ë˜ëŠ” ì·¨ì†Œ ì‹œ None
        """
        try:
            text = request_data.get('message', '')
            streamer_config = request_data.get('streamer_config', {})
            emotion = self._extract_emotion_from_text(text)
            
            logger.info(f"ğŸ¬ ì·¨ì†Œ ê°€ëŠ¥í•œ MediaTrack ìƒì„± ì‹œì‘: {text[:30]}... (ê°ì •: {emotion})")
            
            # AI ì‘ë‹µ ìƒì„± (ìµœìš°ì„ )
            from .llm_text_service import ai_service
            system_prompt = f"ë‹¹ì‹ ì€ '{streamer_config.get('streamer_id', 'AI')}' ìŠ¤íŠ¸ë¦¬ë°ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‹œì²­ìì˜ ì§ˆë¬¸ì— 2-3ì¤„ë¡œ ê°„ê²°í•˜ê³  ì¹œê·¼í•˜ê²Œ ë‹µí•˜ì„¸ìš”. ì‘ë‹µ ëì— ê°ì •ì„ [emotion:happy], [emotion:sad], [emotion:neutral] ë“±ì˜ í˜•íƒœë¡œ ì¶”ê°€í•˜ì„¸ìš”."
            conversation_history = [{"role": "system", "content": system_prompt}]
            
            ai_response = await ai_service.generate_response(text, conversation_history)
            if not ai_response or cancel_event.is_set():
                logger.info("ğŸš« AI ì‘ë‹µ ìƒì„± ì¤‘ ì·¨ì†Œë¨")
                return None
                
            # ê°ì • ì¬ì¶”ì¶œ (AI ì‘ë‹µ ê¸°ë°˜)
            emotion = self._extract_emotion_from_response(ai_response)
            clean_response = self._clean_emotion_tags(ai_response)
            
            # ë³‘ë ¬ MediaTrack ìƒì„± (ì·¨ì†Œ ê°€ëŠ¥) - ì½”ë£¨í‹´ì„ Taskë¡œ ë³€í™˜
            tasks = [
                asyncio.create_task(self._create_audio_track_cancellable(clean_response, streamer_config, cancel_event)),
                asyncio.create_task(self._create_video_track_cancellable(emotion, streamer_config, cancel_event)),
                asyncio.create_task(self._create_subtitle_track_cancellable(clean_response, cancel_event))
            ]
            
            # ëª¨ë“  MediaTrack ì‘ì—… ì™„ë£Œ ëŒ€ê¸° (ì·¨ì†Œ ì²´í¬ì™€ í•¨ê»˜)
            cancel_task = asyncio.create_task(cancel_event.wait())
            all_tasks = tasks + [cancel_task]
            
            done, pending = await asyncio.wait(all_tasks, return_when=asyncio.FIRST_COMPLETED)
            
            # ì·¨ì†Œ ì´ë²¤íŠ¸ê°€ ë¨¼ì € ì™„ë£Œëœ ê²½ìš°
            if cancel_event.is_set():
                # ì·¨ì†Œë¨ - ëª¨ë“  ëŒ€ê¸° ì¤‘ì¸ ì‘ì—…ë“¤ ì •ë¦¬
                for task in tasks:
                    if not task.done():
                        task.cancel()
                logger.info("ğŸš« MediaTrack ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤")
                return None
            
            # ì·¨ì†Œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ëª¨ë“  MediaTrack ì‘ì—… ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
            cancel_task.cancel()  # ì·¨ì†Œ íƒœìŠ¤í¬ ì •ë¦¬
            
            # ë‚¨ì€ MediaTrack ì‘ì—…ë“¤ ì™„ë£Œ ëŒ€ê¸°
            done, pending = await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)
            
            # ì¶”ê°€ ì·¨ì†Œ ì²´í¬
            if cancel_event.is_set():
                logger.info("ğŸš« MediaTrack ìƒì„± ì¤‘ ì·¨ì†Œë¨")
                return None
                
            # ì™„ë£Œëœ íŠ¸ë™ë“¤ ìˆ˜ì§‘
            tracks = []
            for i, task in enumerate(tasks):
                task_name = ['audio', 'video', 'subtitle'][i]  # ìˆœì„œëŒ€ë¡œ ì´ë¦„ ë§¤í•‘
                
                if task.done() and not task.cancelled():
                    try:
                        track = await task
                        if track:
                            tracks.append(track)
                            logger.info(f"âœ… {task_name} íŠ¸ë™ ìƒì„± ì„±ê³µ: {track.kind}, {track.payload_ref[:50]}...")
                        else:
                            logger.warning(f"âš ï¸ {task_name} íŠ¸ë™ ìƒì„± ê²°ê³¼ None")
                    except Exception as e:
                        logger.warning(f"âš ï¸ {task_name} íŠ¸ë™ ìƒì„± ì‹¤íŒ¨: {e}")
                else:
                    if task.cancelled():
                        logger.info(f"ğŸš« {task_name} íŠ¸ë™ ì‘ì—… ì·¨ì†Œë¨")
                    else:
                        logger.warning(f"âš ï¸ {task_name} íŠ¸ë™ ì‘ì—… ë¯¸ì™„ë£Œ")
                        
            if tracks:
                track_kinds = [t.kind for t in tracks]
                logger.info(f"âœ… MediaTrack ìƒì„± ì™„ë£Œ: {len(tracks)}ê°œ íŠ¸ë™ ({', '.join(track_kinds)})")
                return tracks
            else:
                logger.warning("âš ï¸ ìƒì„±ëœ MediaTrackê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
                
        except asyncio.CancelledError:
            logger.info("ğŸš« MediaTrack ìƒì„± ì‘ì—…ì´ ì·¨ì†Œë¨")
            return None
        except Exception as e:
            logger.error(f"âŒ MediaTrack ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return None
    
    async def _create_audio_track_cancellable(self, text: str, streamer_config: Dict, cancel_event: 'asyncio.Event'):
        """ì·¨ì†Œ ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ íŠ¸ë™ ìƒì„±"""
        try:
            if cancel_event.is_set():
                logger.info("ğŸš« ì˜¤ë””ì˜¤ íŠ¸ë™ ìƒì„± ì‹œì‘ ì „ ì·¨ì†Œë¨")
                return None
                
            logger.info(f"ğŸµ ì˜¤ë””ì˜¤ íŠ¸ë™ ìƒì„± ì‹œì‘: {text[:30]}...")
            
            # TTS ìƒì„± (ê¸°ì¡´ ë¡œì§ í™œìš©)
            tts_result = await self._generate_tts_async(text, streamer_config)
            logger.info(f"ğŸ”Š TTS ê²°ê³¼: {tts_result.get('audio_url', 'NO_URL')[:50]}..., ê¸¸ì´: {tts_result.get('duration', 0)}ì´ˆ")
            
            if cancel_event.is_set():
                logger.info("ğŸš« ì˜¤ë””ì˜¤ íŠ¸ë™ TTS ìƒì„± í›„ ì·¨ì†Œë¨")
                return None
                
            # StreamSession.MediaTrack ì„í¬íŠ¸
            from .streaming.domain.stream_session import MediaTrack
            
            duration_ms = int(tts_result['duration'] * 1000)
            
            audio_track = MediaTrack(
                kind="audio",
                pts_ms=0,  # ì¦‰ì‹œ ì‹œì‘
                dur_ms=duration_ms,
                payload_ref=tts_result['audio_url'],
                codec="audio/mpeg",
                meta={
                    'engine': tts_result['tts_info']['engine'],
                    'voice': tts_result['tts_info'].get('voice'),
                    'file_size': tts_result['tts_info'].get('file_size', 0),
                    'emotion': streamer_config.get('emotion', 'neutral')
                }
            )
            
            logger.info(f"âœ… ì˜¤ë””ì˜¤ MediaTrack ìƒì„± ì„±ê³µ: {audio_track.payload_ref[:50]}...")
            return audio_track
            
        except Exception as e:
            if not cancel_event.is_set():
                logger.error(f"âŒ ì˜¤ë””ì˜¤ íŠ¸ë™ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def _create_video_track_cancellable(self, emotion: str, streamer_config: Dict, cancel_event: 'asyncio.Event'):
        """ì·¨ì†Œ ê°€ëŠ¥í•œ ë¹„ë””ì˜¤ íŠ¸ë™ ìƒì„±"""
        try:
            if cancel_event.is_set():
                return None
                
            character_id = streamer_config.get('streamer_id', 'jammin-i')
            talk_video = self.video_selector.get_talk_video(emotion, character_id)
            
            if cancel_event.is_set():
                return None
                
            from .streaming.domain.stream_session import MediaTrack
            
            return MediaTrack(
                kind="video",
                pts_ms=0,  # ì¦‰ì‹œ ì‹œì‘
                dur_ms=5000,  # ê¸°ë³¸ 5ì´ˆ (TTS ê¸¸ì´ì— ë§ì¶¤)
                payload_ref=talk_video,
                codec="video/mp4",
                meta={
                    'emotion': emotion,
                    'character_id': character_id,
                    'clip_type': 'talk'
                }
            )
            
        except Exception as e:
            if not cancel_event.is_set():
                logger.error(f"âŒ ë¹„ë””ì˜¤ íŠ¸ë™ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def _create_subtitle_track_cancellable(self, text: str, cancel_event: 'asyncio.Event'):
        """ì·¨ì†Œ ê°€ëŠ¥í•œ ìë§‰ íŠ¸ë™ ìƒì„±"""
        try:
            if cancel_event.is_set():
                return None
                
            # ìë§‰ íƒ€ì´ë° ìƒì„± (ë¹ ë¥¸ ì‘ì—…)
            subtitle_data = self._generate_subtitle_timing(text, 3.0)  # ê¸°ë³¸ 3ì´ˆ
            
            if cancel_event.is_set():
                return None
                
            from .streaming.domain.stream_session import MediaTrack
            
            return MediaTrack(
                kind="subtitle",
                pts_ms=0,  # ì¦‰ì‹œ ì‹œì‘
                dur_ms=int(subtitle_data['total_duration'] * 1000),
                payload_ref=json.dumps(subtitle_data),  # JSON ë¬¸ìì—´ë¡œ ì €ì¥
                codec="text/json",
                meta=subtitle_data
            )
            
        except Exception as e:
            if not cancel_event.is_set():
                logger.error(f"âŒ ìë§‰ íŠ¸ë™ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_emotion_from_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°„ë‹¨í•œ ê°ì • ì¶”ì¶œ"""
        text_lower = text.lower()
        if any(word in text_lower for word in ['í–‰ë³µ', 'happy', 'ì¢‹ì•„', 'ê¸°ë»', 'ì›ƒìŒ', 'ğŸ˜Š', 'ğŸ˜„']):
            return 'happy'
        elif any(word in text_lower for word in ['ìŠ¬í¼', 'sad', 'ìš°ìš¸', 'ğŸ˜¢', 'ğŸ˜­']):
            return 'sad'
        elif any(word in text_lower for word in ['í™”ë‚˜', 'angry', 'ì§œì¦', 'ğŸ˜ ', 'ğŸ˜¡']):
            return 'angry'
        elif any(word in text_lower for word in ['ë†€ë¼', 'surprised', 'ê¹œì§', 'ğŸ˜±', 'ğŸ˜²']):
            return 'surprised'
        else:
            return 'neutral'
    
    
    async def _generate_tts_async(self, text: str, streamer_config: Dict) -> Dict[str, Any]:
        """TTS ìƒì„± (ë¹„ë™ê¸°)"""
        logger.info(f"ğŸ”Š TTS ìƒì„± ì‹œì‘: {text[:30]}...")
        voice_settings = streamer_config.get('voice_settings', {})
        engine = voice_settings.get('engine', 'elevenlabs')
        logger.info(f"ğŸ”Š TTS ì—”ì§„: {engine}, ì„¤ì •: {voice_settings}")
        
        if engine == 'elevenlabs':
            return await self._generate_elevenlabs_tts(text, voice_settings)
        elif engine == 'openai':
            return await self._generate_openai_tts(text, voice_settings)
        else:
            # ê¸°ë³¸ê°’: ElevenLabs
            return await self._generate_elevenlabs_tts(text, voice_settings)
    
    async def _generate_elevenlabs_tts(self, text: str, voice_settings: Dict) -> Dict[str, Any]:
        """ElevenLabs TTS ìƒì„±"""
        try:
            logger.info(f"ğŸ¤ ElevenLabs TTS í˜¸ì¶œ ì‹œì‘: {text[:30]}...")
            api_key = settings.ELEVENLABS_API_KEY
            if not api_key:
                logger.error("âŒ ElevenLabs API key not configured")
                raise ValueError("ElevenLabs API key not configured")
            
            # í”„ë¡ íŠ¸ì—”ë“œ ì„¤ì • í‚¤ ë§¤í•‘ ì²˜ë¦¬
            voice_id = voice_settings.get('voice_id') or voice_settings.get('elevenLabsVoice', 'aneunjin')
            logger.info(f"ğŸµ ì„ íƒëœ ìŒì„±: {voice_id}")
            
            # ìŒì„± ì„¤ì • ë§¤í•‘ (tts_elevenlabs_service.pyì™€ ë™ì¼í•˜ê²Œ í†µì¼)
            voice_map = {
                'kimtaeri': '6ZND2SlfJqI0OOEHe2by',    # ê¹€íƒœë¦¬ (í•œêµ­ ì—¬ì„± ë°°ìš°)
                'kimminjeong': 'eTiuJAsb9mqCyH5gFsS9', # ê¹€ë¯¼ì • (í•œêµ­ ì—¬ì„± ë°°ìš°)  
                'jinseonkyu': 'pWPHfY5KntyWbx2FxSb7', # ì§„ì„ ê·œ (í•œêµ­ ë‚¨ì„± ë°°ìš°)
                'parkchangwook': 'RQVmMEdMMcmOuv6Fz268', # ë°•ì°½ìš± (í•œêµ­ ë‚¨ì„± ë°°ìš°)
                'aneunjin': 'pRxVZ0v1oH2CqQJWHAty',  # ì•ˆì€ì§„ (í•œêµ­ ì—¬ì„± ë°°ìš°)
                'jiyoung': 'AW5wrnG1jVizOYY7R1Oo'     # JiYoung (í™œê¸°ì°¬ ì Šì€ ì—¬ì„± ìŒì„±) - ì˜¬ë°”ë¥¸ ì—¬ì„± Voice ID
            }
            
            actual_voice_id = voice_map.get(voice_id, voice_map['aneunjin'])
            
            url = f"https://api.elevenlabs.io/v1/text-to-speech/{actual_voice_id}"
            
            headers = {
                "Accept": "audio/mpeg",
                "Content-Type": "application/json",
                "xi-api-key": api_key
            }
            
            # í”„ë¡ íŠ¸ì—”ë“œ ì„¤ì • í‚¤ ë§¤í•‘
            model_id = voice_settings.get('model_id') or voice_settings.get('elevenLabsModel', 'eleven_multilingual_v2')
            stability = voice_settings.get('stability') or voice_settings.get('elevenLabsStability', 0.75)
            similarity = voice_settings.get('similarity_boost') or voice_settings.get('elevenLabsSimilarity', 0.85)
            style = voice_settings.get('style') or voice_settings.get('elevenLabsStyle', 0.0)
            speaker_boost = voice_settings.get('use_speaker_boost') 
            if speaker_boost is None:
                speaker_boost = voice_settings.get('elevenLabsSpeakerBoost', True)
            
            logger.info(f"ğŸ›ï¸ TTS íŒŒë¼ë¯¸í„°: model={model_id}, stability={stability}, similarity={similarity}")
            
            data = {
                "text": text,
                "model_id": model_id,
                "voice_settings": {
                    "stability": stability,
                    "similarity_boost": similarity,
                    "style": style,
                    "use_speaker_boost": speaker_boost
                }
            }
            
            # ë¹„ë™ê¸° HTTP ìš”ì²­
            response = await asyncio.to_thread(requests.post, url, json=data, headers=headers)
            
            if response.status_code != 200:
                logger.error(f"âŒ ElevenLabs API ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                raise Exception(f"ElevenLabs API error: {response.status_code}")
            
            logger.info(f"âœ… ElevenLabs API ì„±ê³µ: {len(response.content)} bytes")
            
            # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ base64ë¡œ ì¸ì½”ë”©í•´ì„œ ì§ì ‘ ì „ë‹¬
            audio_base64 = base64.b64encode(response.content).decode('utf-8')
            audio_data_url = f"data:audio/mpeg;base64,{audio_base64}"
            logger.info(f"ğŸµ ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(response.content)} bytes -> base64")
            
            # ì˜¤ë””ì˜¤ ê¸¸ì´ ì¶”ì • (ì‹¤ì œë¡œëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„ í•„ìš”)
            estimated_duration = len(text) * 0.1  # ëŒ€ëµì  ì¶”ì •
            logger.info(f"â±ï¸ ì¶”ì • ì˜¤ë””ì˜¤ ê¸¸ì´: {estimated_duration}ì´ˆ")
            
            return {
                'audio_url': audio_data_url,
                'duration': estimated_duration,
                'tts_info': {
                    'engine': 'elevenlabs',
                    'voice': voice_id,
                    'model': data['model_id'],
                    'file_size': len(response.content),
                    'format': 'base64_data_url'
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ElevenLabs TTS ì‹¤íŒ¨: {str(e)}")
            # OpenAI í´ë°±
            return await self._generate_openai_tts(text, voice_settings)
    
    async def _generate_openai_tts(self, text: str, voice_settings: Dict) -> Dict[str, Any]:
        """OpenAI TTS ìƒì„± (í´ë°±)"""
        try:
            client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
            
            response = await client.audio.speech.create(
                model="tts-1-hd",
                voice=voice_settings.get('openai_voice', 'alloy'),
                input=text,
                speed=voice_settings.get('speed', 1.0)
            )
            
            # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ base64ë¡œ ì¸ì½”ë”©í•´ì„œ ì§ì ‘ ì „ë‹¬
            audio_base64 = base64.b64encode(response.content).decode('utf-8')
            audio_data_url = f"data:audio/mpeg;base64,{audio_base64}"
            logger.info(f"ğŸµ OpenAI ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(response.content)} bytes -> base64")
            
            estimated_duration = len(text) * 0.08
            
            return {
                'audio_url': audio_data_url,
                'duration': estimated_duration,
                'tts_info': {
                    'engine': 'openai',
                    'voice': voice_settings.get('openai_voice', 'alloy'),
                    'model': 'tts-1-hd',
                    'fallback_used': True,
                    'format': 'base64_data_url'
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ OpenAI TTS í´ë°± ì‹¤íŒ¨: {str(e)}")
            return self._create_tts_error_result(text, str(e))
    
    
    def _generate_subtitle_timing(self, text: str, duration: float) -> Dict[str, Any]:
        """ìë§‰ íƒ€ì´ë° ìƒì„±"""
        words = text.split()
        time_per_word = duration / len(words) if words else 0
        
        subtitle_segments = []
        current_time = 0
        
        for i, word in enumerate(words):
            segment_duration = time_per_word
            subtitle_segments.append({
                'word': word,
                'start': current_time,
                'end': current_time + segment_duration,
                'index': i
            })
            current_time += segment_duration
        
        return {
            'segments': subtitle_segments,
            'total_duration': duration,
            'words_count': len(words)
        }
    
    
    def _create_error_packet(self, text: str, error: str, room_name: str) -> Dict[str, Any]:
        """ì—ëŸ¬ íŒ¨í‚· ìƒì„±"""
        return {
            'sync_id': uuid.uuid4().hex,
            'timestamp': time.time(),
            'room_name': room_name,
            'content': {
                'text': text,
                'error': error,
                'fallback_mode': True
            },
            'sync_timing': {
                'broadcast_delay': 0,
                'processing_time': 0,
                'scheduled_start': time.time()
            },
            'metadata': {
                'error': True,
                'created_at': datetime.now().isoformat()
            }
        }
    
    def _create_tts_error_result(self, text: str, error: str) -> Dict[str, Any]:
        """TTS ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            'audio_url': '',
            'duration': 0,
            'tts_info': {
                'engine': 'none',
                'error': error,
                'fallback_failed': True
            }
        }
    
    def _extract_emotion_from_response(self, response: str) -> str:
        """AI ì‘ë‹µì—ì„œ ê°ì • íƒœê·¸ ì¶”ì¶œ"""
        import re
        emotion_match = re.search(r'\[emotion:(\w+)\]', response)
        if emotion_match:
            return emotion_match.group(1).lower()
        return 'neutral'  # ê¸°ë³¸ê°’
    
    def _clean_emotion_tags(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°ì • íƒœê·¸ ì œê±°"""
        import re
        # [emotion:ê°ì •] í˜•íƒœì˜ íƒœê·¸ ì œê±°
        cleaned = re.sub(r'\[emotion:\w+\]', '', text)
        # ì•ë’¤ ê³µë°± ì œê±°
        return cleaned.strip()